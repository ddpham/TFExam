# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/nlp/01_Word_Embedding.ipynb (unless otherwise specified).

__all__ = ['train_sentences', 'train_labels', 'valid_sentences', 'valid_labels', 'vocab_size', 'embedding_dim',
           'max_len', 'trunc_type', 'oov_tok', 'tokenizer', 'word_index', 'sequences', 'padded_sequences',
           'valid_sequences', 'padded_valid_sequences', 'train_labels', 'valid_labels', 'model', 'history', 'figure',
           'embedding', 'embed_weights', 'out_v', 'out_m', 'sentences', 'labels', 'urls', 'file_name', 'file',
           'num_words', 'embed_dim', 'trunc_type', 'padding_type', 'oov_token', 'training_size', 'train_sentences',
           'valid_sentences', 'train_labels', 'valid_labels', 'tokenizer', 'word_index', 'train_sentences',
           'valid_sentences', 'len_sentences', 'len_dict', 'train_sentences', 'valid_sentences', 'model', 'n',
           'history', 'item_dict', 'plot_list', 'plot_history', 'num_words', 'max_len', 'embed_dim', 'trunc_type',
           'pad_type', 'oov_token', 'training_size', 'sentences', 'labels', 'urls', 'file_name', 'file',
           'train_sentences', 'valid_sentences', 'train_labels', 'valid_labels', 'tokenizer', 'word_index',
           'train_sequences', 'valid_sequences', 'train_labels', 'valid_labels', 'train_sequences', 'valid_sequences',
           'model', 'history', 'sarcasm_emb', 'sarcasm_weights', 'sarc_meta', 'sarc_vect']

# Cell
import tensorflow as tf
import tensorflow.keras as keras
from tensorflow.keras.preprocessing import text
from  tensorflow.keras.preprocessing import sequence
import tensorflow_datasets as tfds
import numpy as np
import json
import matplotlib.pyplot as plt

# Cell
## Download dữ liệu:
imdb, info = tfds.load('imdb_reviews', with_info=True, as_supervised=True)

# Cell
# Tạo tập train, valid:
train_ds, valid_ds = imdb['train'], imdb['test']

# Cell
# Chuyển đổi dữ liệu sang dạng string:
train_sentences = []
train_labels = []
for s, l in train_ds:
    train_sentences.append(str(s.numpy())) # bổ sung từng câu dưới dạng string vào list
    train_labels.append(l.numpy()) # bổ sung nhãn dưới dạng số vào list
print(f'#{len(train_sentences)}:')
print(train_sentences[0])
print(train_labels[0])
print(f'Số lượng labels: {set(train_labels)}')

# Cell
# Làm tương tự với dữ liệu valid:
valid_sentences = []
valid_labels = []

for s, l in valid_ds:
    valid_sentences.append(str(s.numpy()))
    valid_labels.append(l.numpy())
print(f'#{len(valid_sentences)}:')
print(valid_sentences[0])
print(valid_labels[0])

# Cell
vocab_size = 10000
embedding_dim = 16
max_len = 120
trunc_type = 'post'
oov_tok = 'UNK'
# Tạo tokenizer và fit dữ liệu train:
tokenizer = text.Tokenizer(num_words=vocab_size, oov_token=oov_tok)
tokenizer.fit_on_texts(train_sentences)

# Tạo word_index, sequences, pad_sequences từ tập train:
word_index = tokenizer.word_index
sequences = tokenizer.texts_to_sequences(train_sentences)
padded_sequences = sequence.pad_sequences(sequences, maxlen=max_len, padding='post', truncating=trunc_type)

# Tạo sequences, pad_sequences từ tập valid:
valid_sequences = tokenizer.texts_to_sequences(valid_sentences)
padded_valid_sequences = sequence.pad_sequences(valid_sequences, maxlen=max_len, padding='post', truncating=trunc_type)

# Cell
# Biến đổi dữ liệu:
train_labels = np.array(train_labels)
valid_labels = np.array(valid_labels)

# Cell
model = keras.Sequential([
    keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_len)
    , keras.layers.Flatten()
    , keras.layers.Dense(6, activation='relu')
    , keras.layers.Dense(1, activation='sigmoid')
])

model.compile(optimizer=keras.optimizers.Adam(learning_rate=3e-4), loss=keras.losses.BinaryCrossentropy(), metrics=['accuracy'])

model.summary()

# Cell
# Đào tạo:
model.fit(padded_sequences, train_labels, batch_size=64, epochs=10, validation_data=(padded_valid_sequences, valid_labels))

# Cell
history = model.history

# Cell
figure = plt.figure(figsize=(8, 4))
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Loss')
plt.legend(['loss', 'val_loss'])
plt.subplot(1, 2, 2)
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.legend(['accuracy', 'val_accuracy'])
plt.title('Accuracy')
plt.tight_layout()

# Cell
embedding = model.layers[0]
embed_weights = embedding.get_weights()[0]
embed_weights.shape

# Cell
import io
out_v = io.open('vectors.tsv', 'w', encoding='utf-8')
out_m = io.open('meta.tsv', 'w', encoding='utf-8')
# index của từ điển bắt đầu từ 1:
for idx in range(1, vocab_size):
    word = tokenizer.sequences_to_texts([[idx]])[0] # sequences_to_texts có input là list và output là list,
    #nên chúng ta phải để idx dưới dạng list và lấy idx position =0 cho output
    e_w = embed_weights[idx] ## lấy sequence weights của từng từ
    out_m.write(word + '\n') # ghi lại từ
    out_v.write('\t'.join([str(x) for x in e_w]) + '\n')
out_m.close()
out_v.close()

# Cell
sentences = []
labels = []
urls = []

file_name = '/home/ddpham/git/TFExam/data/Sarcasm_Headlines_Dataset.json'
file = open(file_name, 'r')
for line in file.readlines():
    line = json.loads(line)
    sentences.append(line['headline'])
    labels.append(line['is_sarcastic'])
    urls.append(line['article_link'])
file.close()

# Cell
num_words = 10000
embed_dim = 16
# max_len=20
trunc_type='post'
padding_type='post'
oov_token = 'UNK'
training_size = 20000

# Cell
train_sentences = sentences[:training_size]
valid_sentences = sentences[training_size:]
train_labels = np.array(labels[:training_size])
valid_labels = np.array(labels[training_size:])

# Cell
tokenizer = text.Tokenizer(num_words=num_words, oov_token=oov_token)
tokenizer.fit_on_texts(train_sentences)
word_index = tokenizer.word_index
train_sentences = tokenizer.texts_to_sequences(train_sentences)
valid_sentences = tokenizer.texts_to_sequences(valid_sentences)

# Cell
# Chúng ta hay kiểm tra một chút về độ dài của tất cả các câu:
len_sentences = []
for i in train_sentences:
    len_sentences.append(len(i))
len_dict = dict.fromkeys(set(len_sentences), 0)
for length in len_sentences:
    len_dict[length] += 1
len_dict

# Cell
# Padding:
train_sentences = sequence.pad_sequences(train_sentences, padding=padding_type, truncating=trunc_type)
valid_sentences = sequence.pad_sequences(valid_sentences, padding=padding_type, truncating=trunc_type)

# Cell
model = keras.Sequential([
    keras.layers.Embedding(num_words, embed_dim, input_length=max_len)
    , keras.layers.GlobalAveragePooling1D()
    , keras.layers.Dense(24, activation='relu')
    , keras.layers.Dense(1, activation='sigmoid')
])
model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])
model.summary()

# Cell
n = 30
model.fit(train_sentences, train_labels, epochs=n, validation_data=(valid_sentences, valid_labels), verbose=2)

# Cell
history = model.history

# Cell
# Sử dụng loop:
item_dict = {'Loss': ['loss', 'val_loss'], 'Accuracy': ['accuracy', 'val_accuracy']}
plot_list = ['Loss', 'Accuracy']
plt.figure(figsize=(8, 4))
for i in range(len(plot_list)):
    plt.subplot(1, 2, i+1)
    item = plot_list[i]
    for items in item_dict[item]:
        plt.plot(history.history[items])
    plt.legend(item_dict[item])
plt.tight_layout()

# Cell
# Tạo hàm plot:
def plot_history(history):
    item_dict = {'Loss': ['loss', 'val_loss'], 'Accuracy': ['accuracy', 'val_accuracy']}
    plot_list = ['Loss', 'Accuracy']
    plt.figure(figsize=(8, 4))
    for i in range(len(plot_list)):
        plt.subplot(1, 2, i+1)
        item = plot_list[i]
        for items in item_dict[item]:
            plt.plot(history.history[items])
        plt.legend(item_dict[item])
    plt.tight_layout()

# Cell
num_words = 1000 # giảm từ 10K
max_len = 20 # giảm từ 40
embed_dim = 15 # giảm từ 16
trunc_type = 'post'
pad_type = 'post'
oov_token = 'UNK'
training_size = 18000 # tăng size cho tập valid

# Cell
# Parse dữ liệu
sentences = []
labels = []
urls = []

file_name = '/home/ddpham/git/TFExam/data/Sarcasm_Headlines_Dataset.json'
file = open(file_name, 'r')
for line in file.readlines():
    line = json.loads(line)
    sentences.append(line['headline'])
    labels.append(line['is_sarcastic'])
    urls.append(line['article_link'])
file.close()

train_sentences = sentences[:training_size]
valid_sentences = sentences[training_size:]
train_labels = labels[:training_size]
valid_labels = labels[training_size:]

# Cell
# Tokenize & padding:
tokenizer = text.Tokenizer(num_words=num_words, oov_token=oov_token)
# dir(tokenizer)
tokenizer.fit_on_texts(train_sentences)
word_index = tokenizer.word_index
train_sequences = tokenizer.texts_to_sequences(train_sentences)
valid_sequences = tokenizer.texts_to_sequences(valid_sentences)
train_labels = np.array(train_labels)
valid_labels = np.array(valid_labels)

train_sequences = sequence.pad_sequences(train_sequences, maxlen=max_len, padding=pad_type, truncating=trunc_type)
valid_sequences = sequence.pad_sequences(valid_sequences, maxlen=max_len, padding=pad_type, truncating=trunc_type)
# Kiểm tra shape:
train_sequences.shape, valid_sequences.shape

# Cell
model = keras.Sequential([
    keras.layers.Embedding(num_words, embed_dim, input_length=max_len)
    , keras.layers.GlobalAveragePooling1D()
    , keras.layers.Dense(24, activation='relu')
    , keras.layers.Dense(1, activation='sigmoid')
])
model.compile(optimizer='adam', loss='binary_crossentropy', metrics='accuracy')
model.summary()

# Cell
model.fit(train_sequences, train_labels, epochs=10, batch_size=32, validation_data=(valid_sequences, valid_labels))

# Cell
history = model.history

# Cell
# Visualize:
plot_history(history)

# Cell
sarcasm_emb = model.layers[0]
sarcasm_weights = sarcasm_emb.get_weights()[0]
sarcasm_weights.shape

# Cell
sarc_meta = io.open('sarcasm_meta.tsv', 'w', encoding='utf-8')
sarc_vect = io.open('sarcasm_vectors.tsv', 'w', encoding='utf-8')

for idx in range(1, num_words):
    word = tokenizer.sequences_to_texts([[idx]])[0]
    weights = sarcasm_weights[idx]
    sarc_meta.write(word + '\n')
    sarc_vect.write('\t'.join([str(x) for x in weights]) + '\n')
sarc_meta.close()
sarc_vect.close()