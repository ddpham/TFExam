# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/nlp/04_Basic_Text_Classification.ipynb (unless otherwise specified).

__all__ = ['path', 'url', 'dataset', 'path', 'remove_dir', 'train_ds', 'valid_ds', 'text_cleansing', 'max_features',
           'sequence_length', 'vectorization_layer', 'vectorize_text', 'train_ds', 'valid_ds', 'AUTOTUNE', 'train_ds',
           'valid_ds', 'embedding_dim', 'model', 'epochs', 'plot_history', 'test_ds', 'test_ds', 'exp_model', 'test_ds',
           'inference_data', 'path', 'url', 'path', 'bs', 'seed', 'train_ds', 'valid_ds', 'raw_text', 'train_ds',
           'valid_ds', 'train_ds', 'valid_ds', 'embedding_dim', 'model', 'loss', 'optimizer', 'lr_scheduler', 'epochs',
           'history', 'learning_rate', 'embedding_dim', 'model', 'loss', 'optimizer', 'epochs']

# Cell
import tensorflow as tf
import tensorflow.keras as keras
import matplotlib.pyplot as plt
import os, re, shutil, string
import matplotlib.pyplot as plt
%matplotlib inline

# Cell
# Download dữ liệu từ url về path:
path = '/home/ddpham/git/TFExam/data/'
url = 'https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'
dataset = keras.utils.get_file('aclImdb_v1', origin=url, untar=True, cache_dir=path, cache_subdir='')

# Cell
# Tạo lại path:
path = os.path.join(os.path.dirname(dataset), 'aclImdb')

# Cell
# Loại unsup khỏi path:
remove_dir = os.path.join(path, 'train/unsup')
shutil.rmtree(remove_dir)

# Cell
# Tạo tập train và valid từ folder train:
train_ds = keras.preprocessing.text_dataset_from_directory(
    directory=f'{path}/train'
    , labels='inferred'
    , label_mode='binary'
#     , class_names=['neg', 'pos']
    , batch_size=32
    , validation_split=0.2
    , subset='training'
    , seed=42
)
valid_ds = keras.preprocessing.text_dataset_from_directory(
    directory=f'{path}/train'
    , labels = 'inferred'
    , label_mode='binary'
    , batch_size=32
    , validation_split=0.2
    , subset='validation'
    , seed=42
)

# Cell
def text_cleansing(input_text):
    # Convert sang lower case:
    lower_text = tf.strings.lower(input_text)
    # Loại bỏ các ký tự html:
    html_text = tf.strings.regex_replace(lower_text, "<br />", " ")
    # Loại bỏ các dấu:
    puct_text = tf.strings.regex_replace(html_text, "[%s]" % re.escape(string.punctuation), "")
    return puct_text

# Cell
# Tạo layer vectorization:
max_features = 10000
sequence_length = 250

vectorization_layer = keras.layers.experimental.preprocessing.TextVectorization(
    max_tokens=max_features
    , standardize=text_cleansing
    , split='whitespace'
    , ngrams=None
    , output_mode='int'
    , output_sequence_length=sequence_length
)

# Cell
# Gộp toàn bộ dữ liệu text vào:
for _, text in train_ds.enumerate():
    if _ == 0:
        raw_text_train = text[0]
    else: raw_text_train = tf.concat([raw_text_train, text[0]], axis=-1)

# Cell
# Adapt vectorization_layer với dữ liệu text từ tập train:
vectorization_layer.adapt(raw_text_train)

# Cell
# Tạo hàm biến đổi dữ liệu để map vào các tập dataset;
def vectorize_text(text, label):

    # Biến đổi từng comment thành dạng list:
    text = tf.expand_dims(text, -1)
    return vectorization_layer(text), label

# Cell
# Áp dụng với tập train và valid:
train_ds = train_ds.map(vectorize_text)
valid_ds = valid_ds.map(vectorize_text)

# Cell
AUTOTUNE = tf.data.AUTOTUNE # để tf tự quyết định số lượng prefetch dựa vào cấu hình của máy tính
train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)
valid_ds = valid_ds.cache().prefetch(buffer_size=AUTOTUNE)

# Cell
# Tạo kiến trúc cơ bản:
embedding_dim = 64
model = keras.Sequential()

# Tạo embedding layer:
model.add(keras.layers.Embedding(max_features, embedding_dim, input_length=sequence_length))

# Dropout
model.add(keras.layers.Dropout(.2))

# Sử dụng globalaveragepool1D để giảm chiều và lấy
# giá trị average của từng row của matrix embeding
model.add(keras.layers.GlobalAveragePooling1D())

# Tiếp tục dropout:
model.add(keras.layers.Dropout(.2))

# Tạo dense layer để tiếp tục giảm size:
model.add(keras.layers.Dense(10, activation='relu'))

# Tạo layer dự báo:
model.add(keras.layers.Dense(1, activation='sigmoid'))

# Compile model:
model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics='accuracy')
model.summary()

# Cell
# Train model:
epochs = 10
model.fit(train_ds, epochs=epochs, validation_data=valid_ds)

# Cell
def plot_history(history: dict, metrics:str='accuracy'):
    # Tạo list cần plot:
    loss = [i for i in history if 'loss' in i]
    metrics = [i for i in history if f'{metrics}' in i]
    plots = [loss, metrics]

    # Plot:
    fig = plt.figure(figsize=(10, 4))
    for i in range(len(plots)):
        ax = plt.subplot(1, 2, i+1)
        for plot_item in plots[i]:
            ax.plot(history[plot_item])
            ax.legend(plots[i])
            ax.set_xlabel('epochs')
    plt.suptitle("Train's and Validation's Loss vs Metrics")
    plt.tight_layout()

# Cell
test_ds = keras.preprocessing.text_dataset_from_directory(
    directory=f'{path}/test'
    , labels = 'inferred'
)

test_ds = test_ds.map(vectorize_text)

# Cell
print("Đánh giá kết quả trên tập test:")
loss, accuracy = model.evaluate(test_ds)
print("Giá trị loss:", loss)
print("Giá trị accuary:", accuracy)

# Cell
## Kết hợp các layers thành kiến trúc mới
exp_model = keras.Sequential([

    # Layer đầu tiên là biến đổi dữ liệu:
    vectorization_layer,

    # Layer tiếp theo là layer dự đoán kết quả:
    model
])

exp_model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics='accuracy')

# Cell
test_ds = keras.preprocessing.text_dataset_from_directory(
    directory=f'{path}/test'
    , labels = 'inferred'
)


# Kiểm định lại model:
exp_model.evaluate(test_ds)

# Cell
# Tổng hợp lại model:
exp_model.summary()

# Cell
# Thử tài dự đoán:
inference_data = ["This movie is great, it shows compassion and love of human for the nature."
                  , "The content is a mess, I can figure out what's going on even at the end of the movie."
                  , "The movie has both pros and cons. While we see some good acting from the main actor, however, the story line is not up to grasp."]

exp_model.predict(inference_data)

# Cell
path = "/home/ddpham/git/TFExam/data/"
url = "https://storage.googleapis.com/download.tensorflow.org/data/stack_overflow_16k.tar.gz"
keras.utils.get_file(fname='stack_overflow_16k', origin=url, untar=True, cache_dir=path, cache_subdir='stack_overflow_16k')

# Cell
path = os.path.join(os.path.dirname(path), 'stack_overflow_16k')

# Cell
bs = 32
seed=345
train_ds = keras.preprocessing.text_dataset_from_directory(
    directory=f'{path}/train'
    , labels='inferred'
    , label_mode='categorical'
    , batch_size=bs
    , seed=seed
    , validation_split=0.2
    , subset='training'
)
valid_ds = keras.preprocessing.text_dataset_from_directory(
    directory=f"{path}/train"
    , labels='inferred'
    , label_mode='categorical'
    , batch_size=bs
    , seed=seed
    , validation_split=0.2
    , subset='validation'
)

# Cell
# Tạo raw text để adapt vectorization layer:
raw_text = []
for _, text in train_ds.enumerate():
    for ind in range(32):
        raw_text.append([text[0][ind].numpy()])

# Cell
vectorization_layer.adapt(raw_text)

# Cell
train_ds = train_ds.map(vectorize_text)
valid_ds = valid_ds.map(vectorize_text)

# Cell
train_ds = train_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)
valid_ds = valid_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)

# Cell
keras.backend.clear_session()
embedding_dim = 64
model = keras.Sequential([
    keras.layers.Embedding(input_dim=max_tokens, output_dim=embedding_dim, input_length=sequence_length)
    , keras.layers.GlobalMaxPooling1D()
    , keras.layers.Dropout(.5)
    , keras.layers.Dense(20, activation='relu')
    , keras.layers.Dropout(.2)
    , keras.layers.Dense(4, activation='softmax')
])
loss = keras.losses.CategoricalCrossentropy()
optimizer = keras.optimizers.RMSprop(learning_rate=1e-8)
model.compile(optimizer=optimizer, loss=loss, metrics='accuracy')
model.summary()

# Cell
lr_scheduler = keras.callbacks.LearningRateScheduler(lambda epoch: 10e-8 * 10**(epoch/10) )

# Cell
epochs=150
model.fit(train_ds, epochs=epochs, validation_data=valid_ds, callbacks=[lr_scheduler])

# Cell
# Vẽ đồ thị learning rate và loss:
history = model.history
plt.figure(figsize=(8,4))
plt.semilogx(history.history["lr"], history.history["loss"])
plt.axis([1e-8, 1e-1, 0, 2])
plt.show()

# Cell
learning_rate = 5e-3
keras.backend.clear_session()
embedding_dim = 64
model = keras.Sequential([
    keras.layers.Embedding(input_dim=max_tokens, output_dim=embedding_dim, input_length=sequence_length)
    , keras.layers.GlobalMaxPooling1D()
    , keras.layers.Dropout(.5)
    , keras.layers.Dense(20, activation='relu')
    , keras.layers.Dropout(.2)
    , keras.layers.Dense(4, activation='softmax')
])
loss = keras.losses.CategoricalCrossentropy()
optimizer = keras.optimizers.RMSprop(learning_rate=learning_rate)
model.compile(optimizer=optimizer, loss=loss, metrics='accuracy')
model.summary()

# Cell
epochs=50
model.fit(train_ds, epochs=epochs, validation_data=valid_ds)