# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/nlp/02_RNN.ipynb (unless otherwise specified).

__all__ = ['file_name', 'sentences', 'labels', 'num_words', 'oov_tok', 'train_size', 'tokenizer', 'train_sentences',
           'valid_sentences', 'train_labels', 'valid_labels', 'word_index', 'train_sequences', 'valid_sequences',
           'max_len', 'pad_type', 'trunc_type', 'train_sequences', 'valid_sequences', 'embed_dim', 'model', 'epochs',
           'model', 'epochs', 'tokenizer_imdb', 'BUFFER_SIZE', 'BATCH_SIZE', 'train_ds', 'train_ds', 'valid_ds',
           'model', 'epochs', 'plot_history', 'model', 'model']

# Cell
import tensorflow as tf
import tensorflow.keras as keras
import tensorflow.keras.preprocessing.text as text
import tensorflow.keras.preprocessing.sequence as sequence
import tensorflow_datasets as tfds
import matplotlib.pyplot as plt
import numpy as np
import json

# Cell
file_name = '/home/ddpham/git/TFExam/data/Sarcasm_Headlines_Dataset.json'
sentences = []
labels = []
with open(file_name, 'r') as file:
    for line in file.readlines():
        data = json.loads(line)
        sentences.append(data['headline'])
        labels.append(data['is_sarcastic'])
file.close()

# Cell
# Tokenize:
num_words = 1000
oov_tok = 'UNK'
train_size = 20000

tokenizer = text.Tokenizer(num_words=num_words, oov_token=oov_tok)
train_sentences = sentences[:train_size]
valid_sentences = sentences[train_size:]
train_labels = np.array(labels[:train_size])
valid_labels = np.array(labels[train_size:])

tokenizer.fit_on_texts(train_sentences)
word_index = tokenizer.word_index
train_sequences = tokenizer.texts_to_sequences(train_sentences)
valid_sequences = tokenizer.texts_to_sequences(valid_sentences)

# Pad:
max_len = 20
pad_type = 'post'
trunc_type = 'post'

train_sequences = sequence.pad_sequences(train_sequences, maxlen=max_len, padding=pad_type, truncating=trunc_type)
valid_sequences = sequence.pad_sequences(valid_sequences, maxlen=max_len, padding=pad_type, truncating=trunc_type)
train_sequences.shape, valid_sequences.shape

# Cell
# Tạo model đơn giản
embed_dim = 16
model = keras.Sequential([
    keras.layers.Embedding(num_words, embed_dim, input_length=max_len)
    , keras.layers.Bidirectional(keras.layers.LSTM(max_len))
    , keras.layers.Dense(max_len, activation='relu')
    , keras.layers.Dense(1, activation='sigmoid')
])
model.compile(optimizer='adam', loss='binary_crossentropy', metrics='accuracy')
model.summary()

# Cell
epochs=10
model.fit(train_sequences, train_labels, epochs=epochs, validation_data=(valid_sequences, valid_labels))

# Cell
model = keras.Sequential([
    keras.layers.Embedding(num_words, embed_dim, input_length=20),
    # Để có thêm 1 layer nữa của Bidirectional, chúng ta phải thêm return_sequences=T trong LSTM:
    keras.layers.Bidirectional(keras.layers.LSTM(20, return_sequences=True)),
    keras.layers.Bidirectional(keras.layers.LSTM(10)),
    keras.layers.Dense(10, activation='relu'),
    keras.layers.Dense(1, activation='sigmoid')
])
model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])
model.summary()

# Cell
epochs = 10
model.fit(train_sequences, train_labels, epochs=epochs, validation_data=(valid_sequences, valid_labels))

# Cell
import tensorflow_text as tftext

# Cell
# sử dụng review 8k:
dataset, info = tfds.load('imdb_reviews/subwords8k', with_info=True, as_supervised=True)

# Cell
train_ds, valid_ds = dataset['train'], dataset['test']

# Cell
tokenizer_imdb = info.features['text'].encoder

# Cell
BUFFER_SIZE = 10000
BATCH_SIZE = 64

train_ds = train_ds.shuffle(BUFFER_SIZE)
train_ds = train_ds.padded_batch(BATCH_SIZE, tf.compat.v1.data.get_output_shapes(train_ds))
valid_ds = valid_ds.padded_batch(BATCH_SIZE, tf.compat.v1.data.get_output_shapes(valid_ds))

# Cell
model = keras.Sequential([
    keras.layers.Embedding(tokenizer_imdb.vocab_size, 64),
    keras.layers.Bidirectional(keras.layers.LSTM(64, return_sequences=True)),
    keras.layers.Bidirectional(keras.layers.LSTM(32)),
    keras.layers.Dense(20, activation='relu'),
    keras.layers.Dense(1, activation='sigmoid')
])
model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])
model.summary()

# Cell
epochs=5
model.fit(train_ds, epochs=epochs, validation_data=valid_ds)

# Cell
def plot_history(history):
    item_dict = {'Loss': ['loss', 'val_loss'], 'Accuracy': ['accuracy', 'val_accuracy']}
    plot_list = ['Loss', 'Accuracy']
    plt.figure(figsize=(8, 4))
    for i in range(len(plot_list)):
        plt.subplot(1, 2, i+1)
        item = plot_list[i]
        for items in item_dict[item]:
            plt.plot(history.history[items])
        plt.legend(item_dict[item])
    plt.tight_layout()

# Cell
plot_history(model.history)

# Cell
# Với model này, chúng ta sẽ sử dụng lại dữ liệu sarcasm:
model = keras.Sequential([
    keras.layers.Embedding(num_words, embed_dim, input_length=max_len),
    keras.layers.GlobalAveragePooling1D(), #default: pooling_size=2
    keras.layers.Flatten(),
    keras.layers.Dense(24, activation='relu'),
    keras.layers.Dense(1, activation='sigmoid')
])
model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics='accuracy')
model.fit(train_sequences, train_labels, epochs=5, validation_data=(valid_sequences, valid_labels))

# Cell
plot_history(model.history)

# Cell
model = keras.Sequential([
    keras.layers.Embedding(num_words, embed_dim, input_length=max_len),
    keras.layers.Conv1D(128, 5, activation='relu'),
    keras.layers.GlobalMaxPool1D(),
    keras.layers.Flatten(),
    keras.layers.Dense(24, activation='relu'),
    keras.layers.Dense(1, activation='sigmoid')
])
model.compile(optimizer='adam', loss='binary_crossentropy', metrics='accuracy')
model.fit(train_sequences, train_labels, epochs=5, validation_data=(valid_sequences, valid_labels))
plot_history(model.history)