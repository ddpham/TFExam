# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/image/00_image_dataset.ipynb (unless otherwise specified).

__all__ = ['fig', 'X_train', 'X_test', 'normalizer', 'X_train', 'X_test', 'model', 'loss_func', 'rescale_pixel',
           'normalize_img', 'train_ds', 'train_ds', 'valid_ds', 'valid_ds', 'model', 'optimizer', 'loss_func']

# Cell
import tensorflow as tf
import tensorflow_datasets as tfds
import tensorflow.keras as keras
import tensorflow.keras.backend as K
import numpy as np
import matplotlib.pyplot as plt

# Cell
(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()
X_train.shape, y_train.shape

# Cell
# Kiểm tra dữ liệu:
fig = plt.figure(figsize=(3, 3))
for i in range(9):
    ax = plt.subplot(3, 3, i+1)
    plt.imshow(X_train[i,:])
    plt.title(y_train[i])
    plt.axis('off')
    plt.tight_layout()

# Cell
# Biến đổi dữ liệu:
X_train = X_train/255
X_test = X_test/255

# Cell
# Kiểm tra dữ liệu sau khi chia cho 225 (giá trị sẽ từ 0 đến 1)
X_train[1, 10:15, 10:15]

# Cell
# Normalize dữ liệu trước khi train, sử dụng mean và variance của imagenet:
# https://forums.fast.ai/t/is-normalizing-the-input-image-by-imagenet-mean-and-std-really-necessary/51338
normalizer = keras.layers.experimental.preprocessing.Normalization(mean=0.485, variance=0.229**2, name='normalize')
X_train = normalizer(X_train)
X_test = normalizer(X_test)
X_train.shape, X_test.shape

# Cell
# Tạo neural net cơ bản:
model = keras.Sequential([
    keras.layers.Flatten(input_shape=(28, 28), name='flatten'),
    keras.layers.Dense(128, activation='relu', name='layer1'),
    keras.layers.Dropout(.2, name='dropout'),
    keras.layers.Dense(20, activation='relu', name='layer2'),
    keras.layers.Dense(10, activation='softmax', name='predictions')
])
model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Cell
model.summary()

# Cell
# Đào tạo:
model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=5)

# Cell
# tạo hàm loss:
loss_func = keras.losses.SparseCategoricalCrossentropy()
model.compile(optimizer=optimizer, loss=loss_func, metrics='accuracy')

# Cell
# Đào tạo lại
model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=2)

# Cell
model.compile(optimizer=optimizer, loss=loss_func, metrics=['accuracy'])

# Cell
model.summary()

# Cell
model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=1, batch_size=32)

# Cell
train_ds, test_ds = tfds.load('fashion_mnist', split=['train', 'test'], shuffle_files=True, batch_size=32)

# Cell
# Kiểm tra dữ liệu:
plt.figure(figsize=(3, 3))
# Nếu sử dụng batch_size trong hàm load:
for images in train_ds.take(1):
    for i in range(9):
        ax = plt.subplot(3, 3, i + 1)
        plt.imshow(images['image'][i])
        plt.title(images['label'][i].numpy())
        plt.axis("off")
        plt.tight_layout()

# Cell
train_ds, valid_ds = tfds.load('fashion_mnist', split=['train', 'test'], shuffle_files=True, batch_size=64, as_supervised=True)

# Cell
# Kiểm tra shape:
for images in train_ds.take(1):
    print('Number of items:', len(images))
    print('Shape of item 1:', images[0].shape)
    print('Shape of item 2:', images[1].shape)

# Cell
# Kiểm tra dữ liệu:
plt.figure(figsize=(3, 3))
for images in train_ds.take(1):
    for i in range(9):
        ax = plt.subplot(3, 3, i+1)
        plt.imshow(np.repeat(images[0][i], 3, -1))
        plt.title(images[1][i].numpy())
        plt.axis('off')
        plt.tight_layout()

# Cell
def rescale_pixel(image, label):
    image = tf.cast(image, tf.float32)/255.
    return (image, label)

def normalize_img(image, label):
    normalizer = keras.layers.experimental.preprocessing.Normalization(mean=.485, variance=.229**2)
    image = normalizer(image)
    return (image, label)

train_ds = train_ds.map(rescale_pixel)
train_ds = train_ds.map(normalize_img)
valid_ds = valid_ds.map(rescale_pixel)
valid_ds = valid_ds.map(normalize_img)

# Cell
model = keras.Sequential()
model.add(keras.layers.Flatten(input_shape=(28, 28, 1), name='flatten'))
model.add(keras.layers.Dense(128, activation='relu', name='layer1'))
model.add(keras.layers.Dropout(0.5, name='dropout'))
model.add(keras.layers.Dense(20, activation='relu', name='layer2'))
model.add(keras.layers.Dense(10, activation='softmax', name='prediction'))

# Cell
# Create optimizer and loss_fuction
optimizer = keras.optimizers.RMSprop(learning_rate=0.001)
loss_func = keras.losses.SparseCategoricalCrossentropy()
model.compile(optimizer=optimizer, loss=loss_func, metrics='accuracy')

# Cell
model.summary()

# Cell
model.fit(train_ds, epochs=5, validation_data=valid_ds)